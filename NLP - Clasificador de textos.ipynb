{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de textos (clasificador multiclase)\n",
    "En este notebook vamos a crear un clasificador de textos multiclase usando `scikit-learn` sobre modelos BoW, TF-IDF y *averaged word vectors*  \n",
    "Usamos como conjunto de prueba el dataset *20newsgroups* que consiste en unas 18000 noticias en inglés divididas en 20 categorías.  \n",
    "### Descarga del dataset\n",
    "Nos descargamos el dataset y creamos los conjuntos de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargados 18331 documentos\n",
      "Clases:\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Documento de ejemplo:\n",
      " the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "\n",
      "Clase: 19 (talk.religion.misc)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data\n",
    "    \n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "    \n",
    "    \n",
    "dataset = get_data()\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print('\\nCargados {} documentos'.format(len(corpus)))\n",
    "print('Clases:\\n',dataset.target_names)\n",
    "print('\\nDocumento de ejemplo:\\n', corpus[10])\n",
    "print('\\nClase: {} ({})'.format(labels[10], dataset.target_names[labels[10]]))\n",
    "\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de documentos dentro de cada clase está bastante balanceada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       clase    N\n",
       "0                  sci.crypt  689\n",
       "1         rec.sport.baseball  658\n",
       "2             comp.windows.x  690\n",
       "3               misc.forsale  650\n",
       "4         talk.politics.misc  528\n",
       "5      talk.politics.mideast  629\n",
       "6                  rec.autos  649\n",
       "7           rec.sport.hockey  699\n",
       "8      comp.sys.mac.hardware  668\n",
       "9         talk.politics.guns  628\n",
       "10  comp.sys.ibm.pc.hardware  692\n",
       "11           sci.electronics  686\n",
       "12             comp.graphics  684\n",
       "13                   sci.med  684\n",
       "14        talk.religion.misc  406\n",
       "15    soc.religion.christian  661\n",
       "16               alt.atheism  541\n",
       "17   comp.os.ms-windows.misc  652\n",
       "18           rec.motorcycles  672\n",
       "19                 sci.space  665"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([(dataset.target_names[k], v) for k,v in Counter(train_labels).items()], columns=['clase', 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesado del texto\n",
    "Realizamos una limpieza del texto (quitamos signos de puntuación y espacios) y nos quedamos con el lema de cada palabra en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "nlp=spacy.load('en_core_web_md')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    '''Limpieamos y normalizamos un documento\n",
    "    pasado como string'''\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuación/espacios\n",
    "    filtered_tokens = [t for t in tokens if not t.is_punct and not t.is_space and not t.is_digit]\n",
    "    #cogemos el lemma\n",
    "    lemmas = []\n",
    "    for tok in filtered_tokens:\n",
    "        lemmas.append(re.sub('[{}]'.format(re.escape(string.punctuation)), '', tok.lemma_.lower())\n",
    "                      if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    # juntamos de nuevo en una cadena\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    '''Aplicamos la función de normalización sobre\n",
    "    el corpus pasado como lista de string'''\n",
    "    return [normalize_document(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo vemos el documento nº 15 normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following report: _Turkey Eyes Regional Role_ ANKARA, Turkey (AP)\n",
      "April 27, 1993, we find in the last paragraph:\n",
      "\n",
      "[Turanist] Although Premier Suleyman Demirel criticized Ozal's often\n",
      "[Turanist] brash calls for more Turkish influence, he also has spoken\n",
      "[Turanist] of a swath of Turkic peoples \"stretching from the Adriatic\n",
      "[Turanist] Sea to the Great Wall of China.\"\n",
      "\n",
      "Who does Demirel think he is fooling? It seems at both ends of his envisioned \n",
      "pan-Turkic Empire -- the Balkans and the Caucasus -- Turkey's fascist boasts\n",
      "are being pre-empted.\n",
      "\n",
      "I would suggest Turkey let the world feel some of their \"Grey Wolf Teeth\", and\n",
      "attempt to stretch from the Adriatic to China! Turkey will have cried \"wolf\"\n",
      "just once too much! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the following report turkey eyes regional role ankara turkey ap april we find in the last paragraph turanist although premier suleyman demirel criticize ozal s often turanist brash call for more turkish influence he also have speak turanist of a swath of turkic people stretch from the adriatic turanist sea to the great wall of china who do demirel think he be fool it seem at both end of his envision pan turkic empire the balkans and the caucasus turkey s fascist boast be be pre empte i would suggest turkey let the world feel some of their grey wolf teeth and attempt to stretch from the adriatic to china turkey will have cry wolf just once too much\n"
     ]
    }
   ],
   "source": [
    "print(normalize_document(corpus[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos todo el conjunto de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos BoW y TF-IDF\n",
    "Definimos funciones para obtener las características BoW y TF-IDF.  \n",
    "Usamos el parámetro max_df=0.95 para eliminar los stop-words como las palabras que aparecen al menos en el 95% de los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def bow_extractor(corpus):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, max_df=0.95)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "def tfidf_extractor(corpus):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, max_df=0.95)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Averaged Word Vectors  \n",
    "Para calcular los modelos basados en WV primero calculamos los word vectors del CORPUS con la librería `gensim` usando sólo el conjunto de entrenamiento. Con la librería `gensim` es necesario convertir los documentos del corpus en listas de tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return [token.text for token in nlp.tokenizer(text)]\n",
    "\n",
    "\n",
    "procesar = 0 #cambiar a 1 para calcular los vectores, puede tardar un rato\n",
    "\n",
    "if procesar == 0:\n",
    "    # tokenize documentos\n",
    "    tokenized_train = [word_tokenize(text)\n",
    "                       for text in norm_train_corpus]\n",
    "    tokenized_test = [word_tokenize(text)\n",
    "                       for text in norm_test_corpus]\n",
    "    model_vectors = gensim.models.Word2Vec(tokenized_train, \n",
    "                                   size=100,\n",
    "                                   window=10,\n",
    "                                   min_count=2,\n",
    "                                   sample=1e-3)\n",
    "    model = model_vectors.wv\n",
    "    model.save(\"wv_20newsgroup.dict\")\n",
    "\n",
    "model_vectors = gensim.models.KeyedVectors.load(\"wv_20newsgroup.dict\")\n",
    "\n",
    "len(model_vectors.vocab)#nº de palabras en el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01001100b',\n",
       " '1970',\n",
       " '317',\n",
       " 'advisability',\n",
       " 'astonomical',\n",
       " 'attached',\n",
       " 'colorview',\n",
       " 'cronin',\n",
       " 'cts',\n",
       " 'distort',\n",
       " 'eimac',\n",
       " 'exporectx',\n",
       " 'idealist',\n",
       " 'kartonlar',\n",
       " 'm5u',\n",
       " 'mm1',\n",
       " 'normal14',\n",
       " 'pollute',\n",
       " 'problems',\n",
       " 'prompt',\n",
       " 'republish',\n",
       " 'saddlebag',\n",
       " 'stratospheric',\n",
       " 'survellience',\n",
       " 'sysadmin']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "palabras = model_vectors.index2word\n",
    "sorted(np.random.choice(palabras, 25, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos dos modelos basados en word-vectors:  \n",
    "* el vector promedio de los WV de todos los tokens con el mismo peso para todas las palabras.  \n",
    "* ponderando el WV de cada palabra por el término de frecuencia inversa de documento (IDF).  \n",
    "\n",
    "Definimos las funciones para calcular estas dos matrices de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    '''Calcula el vector promedio de los WV de todas las palabras de\n",
    "    un documento pasado como lista de tokens, de acuerdo a:\n",
    "    model: modelo Word2Vec de word vectors\n",
    "    vocabulary: conjunto de palabras en el modelo'''\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    '''Aplica la función de cálculo del WE promedio a todos los\n",
    "    documentos del corpus (cada doc es una lista de tokens)'''\n",
    "    vocabulary = set(model.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    \n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 for word in words]    \n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    vocabulary = set(model.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "                                       \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                   model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de características\n",
    "Extraemos características con los distintos modelos a nuestro conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# características bag of words\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# características tfidf\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)    \n",
    "\n",
    "# características averaged word vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model_vectors,\n",
    "                                                 num_features=100)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model_vectors,\n",
    "                                                num_features=100)                                                 \n",
    "\n",
    "# características tfidf weighted averaged word vector\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model_vectors, \n",
    "                                                                  num_features=100)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model_vectors, \n",
    "                                                                 num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12831, 95573)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12831, 95573)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12831, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12831, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "Aplicamos distintos clasificadores a cada modelo para ver cuál funciona mejor con nuestros datos.  \n",
    "Definimos unas funciones para entrenar y medir el rendimiento de los clasificadores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo.\"\"\"\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos sobre el conjunto de train y evaluamos en el conjunto de test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes con características Bag of Words\n",
      "Accuracy: 0.62\n",
      "Precision: 0.72\n",
      "Recall: 0.62\n",
      "F1 Score: 0.61\n",
      "Support Vector Machine con características Bag of Words\n",
      "Accuracy: 0.66\n",
      "Precision: 0.69\n",
      "Recall: 0.66\n",
      "F1 Score: 0.65\n",
      "Multinomial Naive Bayes con características tf-idf\n",
      "Accuracy: 0.68\n",
      "Precision: 0.74\n",
      "Recall: 0.68\n",
      "F1 Score: 0.67\n",
      "Support Vector Machine con características tf-idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Samsung_T5/opt/anaconda3/envs/tf_2/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.76\n",
      "Recall: 0.77\n",
      "F1 Score: 0.76\n",
      "Support Vector Machine con características averaged word vector\n",
      "Accuracy: 0.44\n",
      "Precision: 0.47\n",
      "Recall: 0.44\n",
      "F1 Score: 0.4\n",
      "Support Vector Machine con características tfidf weighted averaged word vector\n",
      "Accuracy: 0.41\n",
      "Precision: 0.5\n",
      "Recall: 0.41\n",
      "F1 Score: 0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print('Multinomial Naive Bayes con características Bag of Words')\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with bag of words features\n",
    "print('Support Vector Machine con características Bag of Words')\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "                                           \n",
    "# Multinomial Naive Bayes with tfidf features\n",
    "print('Multinomial Naive Bayes con características tf-idf')\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print('Support Vector Machine con características tf-idf')\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print('Support Vector Machine con características averaged word vector')\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "print('Support Vector Machine con características tfidf weighted averaged word vector')\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el mejor modelo es el SVM con características TF-IDF. Vemos su matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>209</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>193</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>197</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>239</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>224</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>258</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>244</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0   129    1    0    2    1    1    3    3    1    2    5    4    4    4    7   \n",
       "1     0  209    7    6    7   11    6    2    2    2    0    2    5    3    4   \n",
       "2     0   19  209   19   11   19    4    0    1    1    1    1    6    0    0   \n",
       "3     0   10   21  193   17    1    9    4    1    1    1    2    6    2    1   \n",
       "4     0    4    6   18  197    4    6    2    3    1    1    2   10    4    1   \n",
       "5     0   21   20    1    3  242    0    0    0    0    0    0    1    0    1   \n",
       "6     0    2    5   12   12    2  239   11    3    2    0    1   11    2    3   \n",
       "7     2    3    2    3    0    2    7  224   19    1    2    0   10    3    3   \n",
       "8     1    0    1    3    4    1    4   28  238    3    1    3    0    1    3   \n",
       "9     3    2    1    0    1    3    4    1    2  258   10    1    2    2    1   \n",
       "10    0    0    2    1    0    0    0    3    1    4  258    0    2    0    1   \n",
       "11    2    6    3    3    1    3    1    0    3    2    0  228    6    1    1   \n",
       "12    0    5    7   18    3    0   12   10    5    3    5    2  188    2    6   \n",
       "13    1    3    2    2    2    3    1    0    1    4    0    0    7  235    4   \n",
       "14    2    5    1    1    1    2    2    4    4    2    1    1    6    3  244   \n",
       "15   12    1    0    0    1    0    0    1    4    2    0    1    1    8    4   \n",
       "16    3    2    0    1    0    1    0    1    4    4    1    8    2    4    2   \n",
       "17    5    0    1    1    0    1    3    2    2    1    2    5    0    3    2   \n",
       "18   12    3    1    2    0    0    0    2    4    4    2    3    0    4    4   \n",
       "19   19    3    1    1    0    4    6    3    4    4    1    3    0    5    1   \n",
       "\n",
       "     15   16   17   18  19  \n",
       "0    40    3   10    6  12  \n",
       "1     2    0    0    2   1  \n",
       "2     0    0    1    3   0  \n",
       "3     0    1    2    0   0  \n",
       "4     0    1    0    1   0  \n",
       "5     1    1    1    0   0  \n",
       "6     0    3    1    0   0  \n",
       "7     1    4    1    1   0  \n",
       "8     0    2    3    1   0  \n",
       "9     3    1    4    0   1  \n",
       "10    1    1    1    1   0  \n",
       "11    2    4    2    5   0  \n",
       "12    4    1    1    0   0  \n",
       "13    5    3    0    3   0  \n",
       "14    2    3    1    5   0  \n",
       "15  269    3    1    3   3  \n",
       "16    1  199    7   16   2  \n",
       "17    5    4  246    5   2  \n",
       "18    4   26    6  149   2  \n",
       "19   58   21    6    5  55  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Matriz de confusión\n",
    "import pandas as pd\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=svm.classes_, columns=svm.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
